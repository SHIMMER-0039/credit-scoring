{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd6f716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss, precision_score, recall_score, f1_score, \\\n",
    "    brier_score_loss, roc_curve, average_precision_score\n",
    "from scipy.stats import ks_2samp\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# VisionPermutator 类定义 (保持不变)\n",
    "class VisionPermutator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.minmax_scaler = MinMaxScaler()\n",
    "        self.robust_scaler = RobustScaler()\n",
    "        self.selector = SelectFromModel(LogisticRegression(penalty=\"l1\", solver=\"liblinear\"))\n",
    "        self.feature_mapping = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X)\n",
    "        self.minmax_scaler.fit(X)\n",
    "        self.robust_scaler.fit(X)\n",
    "        self.selector.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        X_minmax = self.minmax_scaler.transform(X)\n",
    "        X_robust = self.robust_scaler.transform(X)\n",
    "        X_selected = self.selector.transform(X)\n",
    "\n",
    "        # 记录原始特征的映射\n",
    "        self.record_feature_mapping(X, X_scaled, 'scaled')\n",
    "        self.record_feature_mapping(X, X_minmax, 'minmax')\n",
    "        self.record_feature_mapping(X, X_robust, 'robust')\n",
    "        self.record_feature_mapping(X, X_selected, 'selected')\n",
    "\n",
    "        X_selected_expanded = self.expand_selected_features(X_selected, X.shape[1])\n",
    "\n",
    "        X_transformed = self.permute_mlp(pd.DataFrame(X_scaled, columns=X.columns),\n",
    "                                         pd.DataFrame(X_minmax, columns=X.columns),\n",
    "                                         pd.DataFrame(X_robust, columns=X.columns),\n",
    "                                         pd.DataFrame(X_selected_expanded, columns=X.columns))\n",
    "        self.update_feature_mapping(X_transformed, X.columns)\n",
    "        return X_transformed\n",
    "\n",
    "    def record_feature_mapping(self, original_X, transformed_X, method):\n",
    "        for i in range(transformed_X.shape[1]):\n",
    "            original_feature = original_X.columns[i % original_X.shape[1]]\n",
    "            self.feature_mapping[f'{method}_feature_{i}'] = original_feature\n",
    "\n",
    "    def expand_selected_features(self, X_selected, n_features):\n",
    "        expanded_features = np.zeros((X_selected.shape[0], n_features))\n",
    "        selected_indices = np.where(self.selector.get_support())[0]\n",
    "        for i, col_idx in enumerate(selected_indices):\n",
    "            expanded_features[:, col_idx] = X_selected[:, i]\n",
    "        return expanded_features\n",
    "\n",
    "    def permute_mlp(self, X_scaled, X_minmax, X_robust, X_selected):\n",
    "        X_height = self.encode_height(X_scaled, X_minmax, X_robust, X_selected)\n",
    "        X_width = self.encode_width(X_scaled, X_minmax, X_robust, X_selected)\n",
    "        X_weighted = self.weighted_permute_mlp(X_height, X_width)\n",
    "        return X_weighted\n",
    "\n",
    "    def encode_height(self, X_scaled, X_minmax, X_robust, X_selected):\n",
    "        X_height_encoded = X_scaled.apply(lambda row: self.linear_projection(row, axis=0), axis=1)\n",
    "        X_height_encoded += X_minmax.apply(lambda row: self.polynomial_projection(row, axis=0), axis=1)\n",
    "        X_height_encoded += X_robust.apply(lambda row: self.nonlinear_projection(row, axis=0), axis=1)\n",
    "        X_height_encoded += X_selected.apply(lambda row: self.selected_projection(row, axis=0), axis=1)\n",
    "        return pd.DataFrame(X_height_encoded.tolist(), index=X_scaled.index)\n",
    "\n",
    "    def encode_width(self, X_scaled, X_minmax, X_robust, X_selected):\n",
    "        X_width_encoded = X_scaled.apply(lambda row: self.linear_projection(row, axis=1), axis=1)\n",
    "        X_width_encoded += X_minmax.apply(lambda row: self.polynomial_projection(row, axis=1), axis=1)\n",
    "        X_width_encoded += X_robust.apply(lambda row: self.nonlinear_projection(row, axis=1), axis=1)\n",
    "        X_width_encoded += X_selected.apply(lambda row: self.selected_projection(row, axis=1), axis=1)\n",
    "        return pd.DataFrame(X_width_encoded.tolist(), index=X_scaled.index)\n",
    "\n",
    "    def linear_projection(self, row, axis):\n",
    "        projection_vector = np.random.rand(row.shape[0])\n",
    "        if axis == 0:\n",
    "            projection = np.dot(row.values.reshape(-1, 1), projection_vector.reshape(1, -1))\n",
    "        elif axis == 1:\n",
    "            projection = np.dot(projection_vector.reshape(-1, 1), row.values.reshape(1, -1))\n",
    "        projection = projection / np.linalg.norm(projection)  # 正则化\n",
    "        return projection.flatten()\n",
    "\n",
    "    def polynomial_projection(self, row, axis, degree=2):\n",
    "        projection_vector = np.random.rand(row.shape[0])\n",
    "        if axis == 0:\n",
    "            projection = np.dot(row.values.reshape(-1, 1) ** degree, projection_vector.reshape(1, -1))\n",
    "        elif axis == 1:\n",
    "            projection = np.dot(projection_vector.reshape(-1, 1) ** degree, row.values.reshape(1, -1))\n",
    "        projection = projection / np.linalg.norm(projection)  # 正则化\n",
    "        return projection.flatten()\n",
    "\n",
    "    def nonlinear_projection(self, row, axis):\n",
    "        projection_vector = np.random.rand(row.shape[0])\n",
    "        if axis == 0:\n",
    "            projection = np.dot(np.sin(row.values.reshape(-1, 1)), projection_vector.reshape(1, -1))\n",
    "        elif axis == 1:\n",
    "            projection = np.dot(projection_vector.reshape(-1, 1), np.sin(row.values.reshape(1, -1)))\n",
    "        projection = projection / np.linalg.norm(projection)  # 正则化\n",
    "        return projection.flatten()\n",
    "\n",
    "    def selected_projection(self, row, axis):\n",
    "        projection_vector = np.random.rand(row.shape[0])\n",
    "        if axis == 0:\n",
    "            projection = np.dot(row.values.reshape(-1, 1), projection_vector.reshape(1, -1))\n",
    "        elif axis == 1:\n",
    "            projection = np.dot(projection_vector.reshape(-1, 1), row.values.reshape(1, -1))\n",
    "        projection = projection / np.linalg.norm(projection)  # 正则化\n",
    "        return projection.flatten()\n",
    "\n",
    "    def weighted_permute_mlp(self, X_height, X_width):\n",
    "        weight_height = np.random.rand()\n",
    "        weight_width = 1 - weight_height\n",
    "        X_weighted = weight_height * X_height + weight_width * X_width\n",
    "        return X_weighted\n",
    "\n",
    "    def update_feature_mapping(self, X_transformed, original_columns):\n",
    "        for i in range(X_transformed.shape[1]):\n",
    "            original_feature = original_columns[i % len(original_columns)]\n",
    "            self.feature_mapping[f'feature_{i}'] = original_feature\n",
    "\n",
    "def h_mean(precision, recall):\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "def h_mean_score(y_true, y_pred):\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    return h_mean(precision, recall)\n",
    "\n",
    "\n",
    "# 定义 type1error 和 type2error 函数\n",
    "def type1error(y_proba, y_true, threshold=0.5):\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    fp = ((y_pred == 1) & (y_true == 0)).sum()\n",
    "    return fp / (y_true == 0).sum()\n",
    "\n",
    "\n",
    "def type2error(y_proba, y_true, threshold=0.5):\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    fn = ((y_pred == 0) & (y_true == 1)).sum()\n",
    "    return fn / (y_true == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40964118",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = 'D:/study/Credit(1)/Credit/'\n",
    "params_path = r'D:\\study\\Credit(1)\\Credit\\params/'\n",
    "dataset_path = r'D:\\study\\credit_scoring_datasets/'\n",
    "shuffle_path = r'D:\\study\\Credit(1)\\Credit\\shuffle_index/'\n",
    "save_path = r'D:\\study\\second\\outcome/'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "data = pd.read_csv(r'D:\\study\\credit_scroing_datasets\\shandong.csv', low_memory=True)\n",
    "features = data.drop('label', axis=1).replace([-np.inf, np.inf], 0).fillna(0)\n",
    "labels = data['label']\n",
    "\n",
    "# 分割数据集\n",
    "train_size = int(features.shape[0] * 0.8)\n",
    "valid_size = int(features.shape[0] * 0.1)\n",
    "test_size = valid_size  # 假设测试集大小与验证集相同\n",
    "\n",
    "with open(shuffle_path + 'shandong/shuffle_index.pickle', 'rb') as f:\n",
    "    shuffle_index = pickle.load(f)\n",
    "\n",
    "train_index = shuffle_index[:train_size]\n",
    "valid_index = shuffle_index[train_size:(train_size + valid_size)]\n",
    "test_index = shuffle_index[(train_size + valid_size):(train_size + valid_size + test_size)]\n",
    "\n",
    "train_x, train_y = features.iloc[train_index, :], labels.iloc[train_index]\n",
    "valid_x, valid_y = features.iloc[valid_index, :], labels.iloc[valid_index]\n",
    "test_x, test_y = features.iloc[test_index, :], labels.iloc[test_index]\n",
    "\n",
    "full_train_x = pd.concat([train_x, valid_x], axis=0)\n",
    "full_train_y = pd.concat([train_y, valid_y], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e80d7ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = VisionPermutator()\n",
    "vp.fit(full_train_x, full_train_y)\n",
    "full_train_x_transformed = vp.transform(full_train_x)\n",
    "test_x_transformed = vp.transform(test_x)\n",
    "\n",
    "full_train_x_transformed = full_train_x_transformed.astype(float)\n",
    "test_x_transformed = test_x_transformed.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9703dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_param_grid = {\n",
    "    'n_estimators': [500],\n",
    "    'max_depth': [4],\n",
    "    'learning_rate': [0.05],\n",
    "    'reg_lambda': [5]\n",
    "}\n",
    "\n",
    "param_combinations = list(ParameterGrid(lgb_param_grid))\n",
    "results = []\n",
    "positive_count = np.sum(full_train_y == 1)\n",
    "negative_count = np.sum(full_train_y == 0)\n",
    "scale_pos_weight = negative_count / positive_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfd08a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_i = np.mean(full_train_x_transformed, axis=1)  # 示例定义\n",
    "alpha1 = np.exp(gamma_i + 1)  # α1 = exp(γi + 1)\n",
    "alpha2 = 2 - gamma_i          # α2 = 2 - γi\n",
    "import lightgbm as lgb\n",
    "\n",
    "def custom_eval_metric(y_pred, dataset):\n",
    "    y_true = dataset.get_label()\n",
    "    p = 1 / (1 + np.exp(-y_pred))  # sigmoid function\n",
    "    gamma_i = np.mean(full_train_x_transformed, axis=1)\n",
    "    alpha1 = np.exp(gamma_i + 1)\n",
    "    alpha2 = 2 - gamma_i\n",
    "    cost = - (alpha1 * y_true * np.log(p) + alpha2 * (1 - y_true) * np.log(1 - p))\n",
    "    return 'custom_cost', np.mean(cost), False\n",
    "\n",
    "for params_set in param_combinations:\n",
    "    params = {\n",
    "        'n_estimators': params_set['n_estimators'],\n",
    "        'max_depth': params_set['max_depth'],\n",
    "        'learning_rate': params_set['learning_rate'],\n",
    "        'reg_lambda': params_set['reg_lambda'],\n",
    "        'objective': 'binary',\n",
    "        'metric': 'custom'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc32b084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\study software\\Lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "Length of weights differs from the length of #data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m lgb_train \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mDataset(full_train_x_transformed, full_train_y, weight\u001b[38;5;241m=\u001b[39m(alpha1 \u001b[38;5;241m*\u001b[39m full_train_y \u001b[38;5;241m+\u001b[39m alpha2 \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m full_train_y)))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m lgb_model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mtrain(params, lgb_train, valid_sets\u001b[38;5;241m=\u001b[39m[lgb_train], feval\u001b[38;5;241m=\u001b[39mcustom_eval_metric)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 预测和评估模型\u001b[39;00m\n\u001b[0;32m      7\u001b[0m preds_proba \u001b[38;5;241m=\u001b[39m lgb_model\u001b[38;5;241m.\u001b[39mpredict(test_x_transformed)\n",
      "File \u001b[1;32mD:\\study software\\Lib\\site-packages\\lightgbm\\engine.py:255\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# construct booster\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 255\u001b[0m     booster \u001b[38;5;241m=\u001b[39m Booster(params\u001b[38;5;241m=\u001b[39mparams, train_set\u001b[38;5;241m=\u001b[39mtrain_set)\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[0;32m    257\u001b[0m         booster\u001b[38;5;241m.\u001b[39mset_train_data_name(train_data_name)\n",
      "File \u001b[1;32mD:\\study software\\Lib\\site-packages\\lightgbm\\basic.py:3433\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[1;34m(self, params, train_set, model_file, model_str)\u001b[0m\n\u001b[0;32m   3426\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_network(\n\u001b[0;32m   3427\u001b[0m         machines\u001b[38;5;241m=\u001b[39mmachines,\n\u001b[0;32m   3428\u001b[0m         local_listen_port\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_listen_port\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   3429\u001b[0m         listen_time_out\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_out\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m120\u001b[39m),\n\u001b[0;32m   3430\u001b[0m         num_machines\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_machines\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   3431\u001b[0m     )\n\u001b[0;32m   3432\u001b[0m \u001b[38;5;66;03m# construct booster object\u001b[39;00m\n\u001b[1;32m-> 3433\u001b[0m train_set\u001b[38;5;241m.\u001b[39mconstruct()\n\u001b[0;32m   3434\u001b[0m \u001b[38;5;66;03m# copy the parameters from train_set\u001b[39;00m\n\u001b[0;32m   3435\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(train_set\u001b[38;5;241m.\u001b[39mget_params())\n",
      "File \u001b[1;32mD:\\study software\\Lib\\site-packages\\lightgbm\\basic.py:2462\u001b[0m, in \u001b[0;36mDataset.construct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2455\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_init_score_by_predictor(\n\u001b[0;32m   2456\u001b[0m                 predictor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predictor,\n\u001b[0;32m   2457\u001b[0m                 data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata,\n\u001b[0;32m   2458\u001b[0m                 used_indices\u001b[38;5;241m=\u001b[39mused_indices\n\u001b[0;32m   2459\u001b[0m             )\n\u001b[0;32m   2460\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2461\u001b[0m     \u001b[38;5;66;03m# create train\u001b[39;00m\n\u001b[1;32m-> 2462\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_init(data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel, reference\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2463\u001b[0m                     weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, group\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup,\n\u001b[0;32m   2464\u001b[0m                     init_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_score, predictor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predictor,\n\u001b[0;32m   2465\u001b[0m                     feature_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_name, categorical_feature\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_feature,\n\u001b[0;32m   2466\u001b[0m                     params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition)\n\u001b[0;32m   2467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfree_raw_data:\n\u001b[0;32m   2468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\study software\\Lib\\site-packages\\lightgbm\\basic.py:2105\u001b[0m, in \u001b[0;36mDataset._lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, feature_name, categorical_feature, params, position)\u001b[0m\n\u001b[0;32m   2103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel should not be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_weight(weight)\n\u001b[0;32m   2106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2107\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_group(group)\n",
      "File \u001b[1;32mD:\\study software\\Lib\\site-packages\\lightgbm\\basic.py:2925\u001b[0m, in \u001b[0;36mDataset.set_weight\u001b[1;34m(self, weight)\u001b[0m\n\u001b[0;32m   2923\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_pyarrow_array(weight):\n\u001b[0;32m   2924\u001b[0m         weight \u001b[38;5;241m=\u001b[39m _list_to_1d_numpy(weight, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 2925\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_field(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m, weight)\n\u001b[0;32m   2926\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_field(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# original values can be modified at cpp side\u001b[39;00m\n\u001b[0;32m   2927\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mD:\\study software\\Lib\\site-packages\\lightgbm\\basic.py:2682\u001b[0m, in \u001b[0;36mDataset.set_field\u001b[1;34m(self, field_name, data)\u001b[0m\n\u001b[0;32m   2680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m type_data \u001b[38;5;241m!=\u001b[39m _FIELD_TYPE_MAPPER[field_name]:\n\u001b[0;32m   2681\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput type error for set_field\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2682\u001b[0m _safe_call(_LIB\u001b[38;5;241m.\u001b[39mLGBM_DatasetSetField(\n\u001b[0;32m   2683\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle,\n\u001b[0;32m   2684\u001b[0m     _c_str(field_name),\n\u001b[0;32m   2685\u001b[0m     ptr_data,\n\u001b[0;32m   2686\u001b[0m     ctypes\u001b[38;5;241m.\u001b[39mc_int(\u001b[38;5;28mlen\u001b[39m(data)),\n\u001b[0;32m   2687\u001b[0m     ctypes\u001b[38;5;241m.\u001b[39mc_int(type_data)))\n\u001b[0;32m   2688\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversion \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2689\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mD:\\study software\\Lib\\site-packages\\lightgbm\\basic.py:263\u001b[0m, in \u001b[0;36m_safe_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value from C API call.\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;124;03m    The return value from C API calls.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(_LIB\u001b[38;5;241m.\u001b[39mLGBM_GetLastError()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mLightGBMError\u001b[0m: Length of weights differs from the length of #data"
     ]
    }
   ],
   "source": [
    "    lgb_train = lgb.Dataset(full_train_x_transformed, full_train_y, weight=(alpha1 * full_train_y + alpha2 * (1 - full_train_y)))\n",
    "\n",
    "    # 训练模型\n",
    "    lgb_model = lgb.train(params, lgb_train, valid_sets=[lgb_train], feval=custom_eval_metric)\n",
    "\n",
    "    # 预测和评估模型\n",
    "    preds_proba = lgb_model.predict(test_x_transformed)\n",
    "    preds = (preds_proba >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19424726",
   "metadata": {},
   "outputs": [],
   "source": [
    " auc_score = roc_auc_score(test_y, preds_proba)\n",
    "    logloss = log_loss(test_y, preds_proba)\n",
    "    ks = ks_2samp(preds_proba[test_y == 1], preds_proba[test_y != 1]).statistic\n",
    "    accuracy = accuracy_score(test_y, preds)\n",
    "    precision = precision_score(test_y, preds)\n",
    "    recall = recall_score(test_y, preds)\n",
    "    f1 = f1_score(test_y, preds)\n",
    "    brier_score = brier_score_loss(test_y, preds_proba)\n",
    "    average_precision = average_precision_score(test_y, preds_proba)\n",
    "    fprs, tprs, thresholds = roc_curve(test_y, preds_proba)\n",
    "    true_positive_rate = tprs\n",
    "    true_negative_rate = 1 - fprs\n",
    "    gmean = np.sqrt(true_positive_rate * true_negative_rate)\n",
    "\n",
    "    # 计算 H-mean 和其他自定义指标\n",
    "    hm = h_mean(precision, recall)\n",
    "\n",
    "    # 计算 type1error 和 type2error\n",
    "    type1_error = type1error(preds_proba, test_y)\n",
    "    type2_error = type2error(preds_proba, test_y)\n",
    "\n",
    "    # 计算 Acc AUC Prec Rec 的平均值\n",
    "    average_score = (accuracy + auc_score + precision + recall) / 4\n",
    "\n",
    "    # 将结果存入列表\n",
    "    results.append({\n",
    "        'params': params_set,\n",
    "        'accuracy': accuracy,\n",
    "        'auc_score': auc_score,\n",
    "        'logloss': logloss,\n",
    "        'ks_stat': ks,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'brier_score': brier_score,\n",
    "        'average_precision': average_precision,\n",
    "        'hm': hm,\n",
    "        'gmean': gmean,\n",
    "        'type1_error': type1_error,\n",
    "        'type2_error': type2_error,\n",
    "        'average_score': average_score\n",
    "    })\n",
    "\n",
    "    # 输出每个参数组合的结果\n",
    "    print(f\"Params: {params_set}, Accuracy: {accuracy}, AUC: {auc_score}, Average Score: {average_score}\")\n",
    "\n",
    "# 输出所有结果\n",
    "print(\"所有结果：\")\n",
    "for result in results:\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
