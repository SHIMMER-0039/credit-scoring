{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74ae8b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss, precision_score, recall_score, f1_score, \\\n",
    "    brier_score_loss, roc_curve, average_precision_score\n",
    "from scipy.stats import ks_2samp\n",
    "from xgboost import XGBClassifier  # 修改为 XGBoost\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import os\n",
    "from lightgbm import LGBMClassifier\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss, precision_score, recall_score, f1_score, \\\n",
    "    brier_score_loss, roc_curve, average_precision_score\n",
    "from scipy.stats import ks_2samp\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import os\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "# VisionPermutator 类定义 (保持不变)\n",
    "class VisionPermutator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.minmax_scaler = MinMaxScaler()\n",
    "        self.robust_scaler = RobustScaler()\n",
    "        self.selector = SelectFromModel(LogisticRegression(penalty=\"l1\", solver=\"liblinear\"))\n",
    "        self.feature_mapping = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X)\n",
    "        self.minmax_scaler.fit(X)\n",
    "        self.robust_scaler.fit(X)\n",
    "        self.selector.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        X_minmax = self.minmax_scaler.transform(X)\n",
    "        X_robust = self.robust_scaler.transform(X)\n",
    "        X_selected = self.selector.transform(X)\n",
    "\n",
    "        # 记录原始特征的映射\n",
    "        self.record_feature_mapping(X, X_scaled, 'scaled')\n",
    "        self.record_feature_mapping(X, X_minmax, 'minmax')\n",
    "        self.record_feature_mapping(X, X_robust, 'robust')\n",
    "        self.record_feature_mapping(X, X_selected, 'selected')\n",
    "\n",
    "        X_selected_expanded = self.expand_selected_features(X_selected, X.shape[1])\n",
    "\n",
    "        X_transformed = self.permute_mlp(pd.DataFrame(X_scaled, columns=X.columns),\n",
    "                                         pd.DataFrame(X_minmax, columns=X.columns),\n",
    "                                         pd.DataFrame(X_robust, columns=X.columns),\n",
    "                                         pd.DataFrame(X_selected_expanded, columns=X.columns))\n",
    "        self.update_feature_mapping(X_transformed, X.columns)\n",
    "        return X_transformed\n",
    "\n",
    "    def record_feature_mapping(self, original_X, transformed_X, method):\n",
    "        for i in range(transformed_X.shape[1]):\n",
    "            original_feature = original_X.columns[i % original_X.shape[1]]\n",
    "            self.feature_mapping[f'{method}_feature_{i}'] = original_feature\n",
    "\n",
    "    def expand_selected_features(self, X_selected, n_features):\n",
    "        expanded_features = np.zeros((X_selected.shape[0], n_features))\n",
    "        selected_indices = np.where(self.selector.get_support())[0]\n",
    "        for i, col_idx in enumerate(selected_indices):\n",
    "            expanded_features[:, col_idx] = X_selected[:, i]\n",
    "        return expanded_features\n",
    "\n",
    "    def permute_mlp(self, X_scaled, X_minmax, X_robust, X_selected):\n",
    "        X_height = self.encode_height(X_scaled, X_minmax, X_robust, X_selected)\n",
    "        X_width = self.encode_width(X_scaled, X_minmax, X_robust, X_selected)\n",
    "        X_weighted = self.weighted_permute_mlp(X_height, X_width)\n",
    "        return X_weighted\n",
    "\n",
    "    def encode_height(self, X_scaled, X_minmax, X_robust, X_selected):\n",
    "        X_height_encoded = X_scaled.apply(lambda row: self.linear_projection(row, axis=0), axis=1)\n",
    "        X_height_encoded += X_minmax.apply(lambda row: self.polynomial_projection(row, axis=0), axis=1)\n",
    "        X_height_encoded += X_robust.apply(lambda row: self.nonlinear_projection(row, axis=0), axis=1)\n",
    "        X_height_encoded += X_selected.apply(lambda row: self.selected_projection(row, axis=0), axis=1)\n",
    "        return pd.DataFrame(X_height_encoded.tolist(), index=X_scaled.index)\n",
    "\n",
    "    def encode_width(self, X_scaled, X_minmax, X_robust, X_selected):\n",
    "        X_width_encoded = X_scaled.apply(lambda row: self.linear_projection(row, axis=1), axis=1)\n",
    "        X_width_encoded += X_minmax.apply(lambda row: self.polynomial_projection(row, axis=1), axis=1)\n",
    "        X_width_encoded += X_robust.apply(lambda row: self.nonlinear_projection(row, axis=1), axis=1)\n",
    "        X_width_encoded += X_selected.apply(lambda row: self.selected_projection(row, axis=1), axis=1)\n",
    "        return pd.DataFrame(X_width_encoded.tolist(), index=X_scaled.index)\n",
    "\n",
    "    def linear_projection(self, row, axis):\n",
    "        projection_vector = np.random.rand(row.shape[0])\n",
    "        if axis == 0:\n",
    "            projection = np.dot(row.values.reshape(-1, 1), projection_vector.reshape(1, -1))\n",
    "        elif axis == 1:\n",
    "            projection = np.dot(projection_vector.reshape(-1, 1), row.values.reshape(1, -1))\n",
    "        projection = projection / np.linalg.norm(projection)  # 正则化\n",
    "        return projection.flatten()\n",
    "\n",
    "    def polynomial_projection(self, row, axis, degree=2):\n",
    "        projection_vector = np.random.rand(row.shape[0])\n",
    "        if axis == 0:\n",
    "            projection = np.dot(row.values.reshape(-1, 1) ** degree, projection_vector.reshape(1, -1))\n",
    "        elif axis == 1:\n",
    "            projection = np.dot(projection_vector.reshape(-1, 1) ** degree, row.values.reshape(1, -1))\n",
    "        projection = projection / np.linalg.norm(projection)  # 正则化\n",
    "        return projection.flatten()\n",
    "\n",
    "    def nonlinear_projection(self, row, axis):\n",
    "        projection_vector = np.random.rand(row.shape[0])\n",
    "        if axis == 0:\n",
    "            projection = np.dot(np.sin(row.values.reshape(-1, 1)), projection_vector.reshape(1, -1))\n",
    "        elif axis == 1:\n",
    "            projection = np.dot(projection_vector.reshape(-1, 1), np.sin(row.values.reshape(1, -1)))\n",
    "        projection = projection / np.linalg.norm(projection)  # 正则化\n",
    "        return projection.flatten()\n",
    "\n",
    "    def selected_projection(self, row, axis):\n",
    "        projection_vector = np.random.rand(row.shape[0])\n",
    "        if axis == 0:\n",
    "            projection = np.dot(row.values.reshape(-1, 1), projection_vector.reshape(1, -1))\n",
    "        elif axis == 1:\n",
    "            projection = np.dot(projection_vector.reshape(-1, 1), row.values.reshape(1, -1))\n",
    "        projection = projection / np.linalg.norm(projection)  # 正则化\n",
    "        return projection.flatten()\n",
    "\n",
    "    def weighted_permute_mlp(self, X_height, X_width):\n",
    "        weight_height = np.random.rand()\n",
    "        weight_width = 1 - weight_height\n",
    "        X_weighted = weight_height * X_height + weight_width * X_width\n",
    "        return X_weighted\n",
    "\n",
    "    def update_feature_mapping(self, X_transformed, original_columns):\n",
    "        for i in range(X_transformed.shape[1]):\n",
    "            original_feature = original_columns[i % len(original_columns)]\n",
    "            self.feature_mapping[f'feature_{i}'] = original_feature\n",
    "\n",
    "def h_mean(precision, recall):\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "def h_mean_score(y_true, y_pred):\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    return h_mean(precision, recall)\n",
    "\n",
    "\n",
    "# 定义 type1error 和 type2error 函数\n",
    "def type1error(y_proba, y_true, threshold=0.5):\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    fp = ((y_pred == 1) & (y_true == 0)).sum()\n",
    "    return fp / (y_true == 0).sum()\n",
    "\n",
    "\n",
    "def type2error(y_proba, y_true, threshold=0.5):\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    fn = ((y_pred == 0) & (y_true == 1)).sum()\n",
    "    return fn / (y_true == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "235ed9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = 'D:/study/Credit(1)/Credit/'\n",
    "params_path = r'D:\\study\\Credit(1)\\Credit\\params/'\n",
    "dataset_path = r'D:\\study\\credit_scoring_datasets/'\n",
    "shuffle_path = r'D:\\study\\Credit(1)\\Credit\\shuffle_index/'\n",
    "save_path = r'D:\\study\\second\\outcome/'\n",
    "os.makedirs(save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94be7ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'D:\\study\\credit_scroing_datasets\\lending club/2012-2013.csv', low_memory=True)\n",
    "features = data.drop('loan_status', axis=1)\n",
    "labels = data['loan_status']\n",
    "\n",
    "# 分割数据集\n",
    "train_size = int(features.shape[0] * 0.8)\n",
    "valid_size = int(features.shape[0] * 0.1)\n",
    "test_size = valid_size  # 假设测试集大小与验证集相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8028c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(features.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_size = int(features.shape[0] * 0.8)\n",
    "valid_size = int(features.shape[0] * 0.1)\n",
    "test_size = valid_size  # 假设测试集大小与验证集相同\n",
    "\n",
    "train_index = indices[:train_size]\n",
    "valid_index = indices[train_size:(train_size + valid_size)]\n",
    "test_index = indices[(train_size + valid_size):(train_size + valid_size + test_size)]\n",
    "remaining_index = indices[(train_size + valid_size + test_size):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6059f568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data size: 235314\n",
      "Train indices: [170876 123595 163396  26736 200744]...[215776   9427  48190  10948 186052]\n",
      "Valid indices: [ 81392  51566 147274  12036 152888]...[153679 196033 153317 115524 172132]\n",
      "Test indices: [159887  29238 148633 180351  15328]...[224443 137347 129485 123175    879]\n",
      "Remaining indices: [107062]...[107062]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total data size: {features.shape[0]}\")\n",
    "print(f\"Train indices: {train_index[:5]}...{train_index[-5:]}\")\n",
    "print(f\"Valid indices: {valid_index[:5]}...{valid_index[-5:]}\")\n",
    "print(f\"Test indices: {test_index[:5]}...{test_index[-5:]}\")\n",
    "print(f\"Remaining indices: {remaining_index[:5]}...{remaining_index[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "055b0691",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = features.iloc[train_index, :], labels.iloc[train_index]\n",
    "valid_x, valid_y = features.iloc[valid_index], labels.iloc[valid_index]\n",
    "test_x, test_y = features.iloc[test_index], labels.iloc[test_index]\n",
    "remaining_x, remaining_y = features.iloc[remaining_index], labels.iloc[remaining_index]\n",
    "full_train_x = pd.concat([train_x, valid_x], axis=0)\n",
    "full_train_y = pd.concat([train_y, valid_y], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ce7f5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = VisionPermutator()\n",
    "vp.fit(full_train_x, full_train_y)\n",
    "full_train_x_transformed = vp.transform(full_train_x)\n",
    "test_x_transformed = vp.transform(test_x)\n",
    "\n",
    "# 确保转换后的数据为数值型\n",
    "full_train_x_transformed = full_train_x_transformed.astype(float)\n",
    "test_x_transformed = test_x_transformed.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68f11d2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgb_param_grid = {\n",
    "    'n_estimators': [300, 500, 700],\n",
    "    'max_depth': [2, 4, 6, 8],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'reg_lambda': [3, 5, 7],\n",
    "    'subsample': [0.5, 0.7, 0.9]\n",
    "}\n",
    "\n",
    "\n",
    "# 获取所有参数组合\n",
    "param_combinations = list(ParameterGrid(lgb_param_grid))\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cf7a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "for params_set in param_combinations:\n",
    "    params = {\n",
    "        'n_estimators': params_set['n_estimators'],\n",
    "        'learning_rate': params_set['learning_rate'],\n",
    "        'max_depth': params_set['max_depth'],\n",
    "        'num_leaves': 2**params_set['max_depth'] - 1,\n",
    "        'reg_lambda': params_set['reg_lambda'],\n",
    "        'subsample': params_set['subsample'],\n",
    "        'objective': 'binary',\n",
    "        'verbosity': 0\n",
    "    }\n",
    "\n",
    "    # 训练模型\n",
    "    lgb_model = LGBMClassifier(**params)\n",
    "    lgb_model.fit(full_train_x_transformed, full_train_y)\n",
    "\n",
    "    # 预测和评估模型\n",
    "    preds_proba = lgb_model.predict_proba(test_x_transformed)[:, 1]\n",
    "    preds = lgb_model.predict(test_x_transformed)\n",
    "\n",
    "    # 计算评估指标\n",
    "    auc_score = roc_auc_score(test_y, preds_proba)\n",
    "    logloss = log_loss(test_y, preds_proba)\n",
    "    ks = ks_2samp(preds_proba[test_y == 1], preds_proba[test_y != 1]).statistic\n",
    "    accuracy = accuracy_score(test_y, preds)\n",
    "    precision = precision_score(test_y, preds)\n",
    "    recall = recall_score(test_y, preds)\n",
    "    f1 = f1_score(test_y, preds)\n",
    "    brier_score = brier_score_loss(test_y, preds_proba)\n",
    "    average_precision = average_precision_score(test_y, preds_proba)\n",
    "    fprs, tprs, thresholds = roc_curve(test_y, preds_proba)\n",
    "    true_positive_rate = tprs\n",
    "    true_negative_rate = 1 - fprs\n",
    "#     gmean = np.sqrt(true_positive_rate * true_negative_rate)\n",
    "\n",
    "    # 计算 H-mean 和其他自定义指标\n",
    "    hm = h_mean(precision, recall)\n",
    "\n",
    "    # 计算 type1error 和 type2error\n",
    "    type1_error = type1error(preds_proba, test_y)\n",
    "    type2_error = type2error(preds_proba, test_y)\n",
    "\n",
    "    # 计算 Acc AUC Prec Rec 的平均值\n",
    "    average_score = (accuracy + auc_score + precision + recall) / 4\n",
    "\n",
    "    # 将结果存入列表\n",
    "    results.append({\n",
    "        'params': params_set,\n",
    "        'accuracy': accuracy,\n",
    "        'auc_score': auc_score,\n",
    "        'logloss': logloss,\n",
    "        'ks_stat': ks,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'brier_score': brier_score,\n",
    "        'average_precision': average_precision,\n",
    "        'hm': hm,\n",
    "#         'gmean': gmean,\n",
    "        'type1_error': type1_error,\n",
    "        'type2_error': type2_error,\n",
    "        'average_score': average_score\n",
    "    })\n",
    "    print(\"所有结果：\")\n",
    "    for result in results:\n",
    "        print(result)\n",
    "    # 输出每个参数组合的结果\n",
    "\n",
    "    \n",
    "    print(f\"Params: {params_set}, Accuracy: {accuracy}, AUC: {auc_score}, Average Score: {average_score}\")\n",
    "\n",
    "# 输出所有结果\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7936c213",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = max(results, key=lambda x: x['average_score'])\n",
    "print(\"最佳结果：\", best_result)\n",
    "\n",
    "# 保存所有结果到字典并写入文件\n",
    "results_dict = {'results': results, 'best_result': best_result}\n",
    "\n",
    "dataset = 'bankfear'\n",
    "method = 'VIP'\n",
    "file_path = os.path.join(save_path, f'{dataset}\\\\{method}_res.pickle')\n",
    "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(results_dict, f)\n",
    "\n",
    "print(f'This is ACC: {best_result[\"accuracy\"]}')\n",
    "print(f'This is AUC: {best_result[\"auc_score\"]}')\n",
    "print(f'The results of {method} on {dataset} have been calculated and saved.\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3f67fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
